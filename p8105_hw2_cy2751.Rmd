---
title: "p8105_hw2_cy2751"
author: "Congyu Yang"
date: "2024-09-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(knitr)
```

## Problem 1

```{r}
NYCTransit_df <- read_csv(
  "datasets_hw2/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",na = "",
  col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(line,station_name,station_latitude,station_longitude,(route1:route11),
         entry,exit_only,vending,entrance_type,ada) %>% 
  mutate(entry = ifelse(entry == "YES",T,F))
```
This dataset contains information of all train stations in NYC. Including variables train line, station name, the specific location of the station( latitude and longitude),all routes inside each station, entry and exit information, vending information, entrance type and whether ada compliance or not. I clean the data by importing data, identify `NA` values, specify the variable type of all routes. I also update variables names and select only the useful variables for the later question. Lastly, update variable `entry` to be a logical variable. Current dataset has dimension 1,868 Ã— 20.\
We cannot say these data are tidy, because when we need to focus on specific routes, routes should be a variable rather than a column name.\

```{r}
NYCTransit_df %>%  
  select(station_name, line) %>%  
  distinct() 
```
There are 465 distinct stations.

```{r}
NYCTransit_df %>% 
  select(station_name,line,ada) %>% 
  distinct() %>% 
  filter(ada == T) 
```
In all, 84 stations are ADA compliant.

```{r}
NYCTransit_df %>%
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean()
```

Around 37.7% of station entrances/exit with vending allow entrance. 

```{r}
NYCTransit_df %>% 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) %>% 
  filter(route_name == "A") %>% 
  select(line,station_name) %>% 
    distinct()

NYCTransit_df %>% 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  ) %>% 
  filter(route_name == "A") %>% 
  select(line,station_name,ada) %>% 
  distinct() %>% 
    filter(ada == T)
  
```
60 distinct stations serve A train. Among these stations, 17 of them are ADA compliant.



## Problem 2

```{r}
Mr_Trash_df <- read_excel("datasets_hw2/202409 Trash Wheel Collection Data.xlsx",
                          sheet = "Mr. Trash Wheel",range = "A2:N653") %>% 
  janitor::clean_names() %>%
  mutate(sports_balls=as.integer(round(sports_balls))) %>% 
  mutate(year = as.numeric(year))

Professor_Trash_df <- read_excel("datasets_hw2/202409 Trash Wheel Collection Data.xlsx",
                          sheet = "Professor Trash Wheel",range = "A2:M120") %>% 
  janitor::clean_names()

Gwynnda_Trash_df <- read_excel("datasets_hw2/202409 Trash Wheel Collection Data.xlsx",
                          sheet = "Gwynnda Trash Wheel",range = "A2:L265") %>% 
  janitor::clean_names()

Mr_Trash_Bind <- Mr_Trash_df %>% mutate(name = "Mr_Trash") %>% 
  select(name,everything())

Professor_Trash_Bind <- Professor_Trash_df %>% mutate(name = "Professor_Trash") %>% 
  select(name,everything())

Gwynnda_Trash_Bind <- Gwynnda_Trash_df %>% mutate(name = "Gwynnda_Trash") %>% 
  select(name,everything())

(Whole_Trash_Bind <- bind_rows(Mr_Trash_Bind,Professor_Trash_Bind,Gwynnda_Trash_Bind) %>%
  select(-month,-year)) 
# remove month and year for tidy reason, date already includes such information
```
We combined three datasets: `Mr_Trash` dataset, `Professor_Trash` dataset and `Gwynnda_Trash` dataset. Each contains information about every dumpsters, including the date we collected data on, weight of trash it deals with in ton, volume of trash and amount of each kind of trash such as plastic bottles,polystyrene.... It is noteworthy that sports ball is a unique trash that only exists in `Mr_Trash` dataset and there is no glass bottle inside `Gwynnda_Trash` dataset. \
For `Mr_Trash` dataset, we collected information from `r nrow(Mr_Trash_df)` dumpsters and the information is between 2014 and 2024 time period;\

For `Professor_Trash` dataset, we collected information from `r nrow(Professor_Trash_df)` dumpsters and the information is between 2017 to 2024 time period;\

For `Gwynnda_Trash` dataset, we have information for 262 dumpsters, while we have `r nrow(Gwynnda_Trash_df)` pieces of information, it is because we have two rows for dumpster 21, but on the different date, these information is between 2021 and 2024;\

The final dataset has `r nrow(Whole_Trash_Bind)` observations in total. For each observation, we have 13 variables. We removed `month` and `year` variable in the final dataset because these are redundant variables, we can also see these information in `date` variable. \
We added a new column `name` to the final dataset, which indicates which Trash Wheel does this dumpster come from. 

```{r}
Professor_Trash_df %>% select(weight_tons) %>% sum()
```
The total weight of trash collected by Professor Trash Wheel is `r Professor_Trash_df %>% select(weight_tons) %>% sum()` tons.\

```{r}
Gwynnda_Trash_df %>% 
  filter(month == "June", year == "2022") %>% 
  select(cigarette_butts) %>% 
  sum()
```
The total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.\

## Problem 3

```{r}
bakers <- read_csv("datasets_hw2/gbb_datasets/bakers.csv") %>% 
  janitor::clean_names()%>% 
  separate(baker_name,into = c("baker","last_name"),sep = " ")

bakes <- read_csv("datasets_hw2/gbb_datasets/bakes.csv") %>% 
  janitor::clean_names() %>% 
  mutate(baker=gsub('\"', '',baker))

results <- read_csv("datasets_hw2/gbb_datasets/results.csv",skip =2) %>% 
  janitor::clean_names()%>% 
  mutate(
    baker = case_when(baker == "Joanne" ~ "Jo",TRUE ~ baker)
    )

anti_join(bakers,bakes,by = "baker") # bakers in series 9 and 10 does not exist in bakes dataset
anti_join(bakes,bakers,by = "baker")
anti_join(bakers, results, by = "baker")
anti_join(results, bakers, by = "baker")
anti_join(bakes,results,by = "baker")
anti_join(results,bakes,by = "baker") # verify bakes has information only until series 8

(results_and_bakers <- left_join(results,bakers,by = c("baker","series")))
(bakers_all_info <- left_join(results_and_bakers,bakes,by = c("series","episode","baker")))

(bakers_all_info_tidy <- 
    bakers_all_info %>%
    mutate(baker_name = str_c(baker,last_name,sep = " ")) %>% 
    select(series,episode,baker_name,
           signature_bake,show_stopper,technical,
           result,everything(),-baker,-last_name) %>% 
  filter(!is.na(result)))
```

For the data cleaning process, I first imported all three datasets and clean their names. Separating bakers' full name into `first name` and `last name`. Since in the other two datasets, we only use their first name. And then, by viewing individual datasets and using `anti-join` function, checking bakers' information in between any two of the datasets, for example: to see if there is any bakers inside `bakers` dataset but not in `bakes` dataset, here are my findings:\
There are 23 bakers appears in `bakers` dataset but not inside `bakes` dataset. It is because `bakes` only contains information from series 1 - 8, so it is reasonable that some bakers only exists in `bakers` dataset but not inside `bakes` dataset, like bakers attending series 9 and 10 . There is one exception: `"Jo"`, which matches our result in checking bakers in `bakes` that are not in `bakers`. We should remove the quotation mark for this cell in order for it to match.\

By applying `anti_join` twice on `bakers` and `results`, we can see the bakers `Joanne` may use her name `Jo` to apply for the competition, because it is the only situation that does not match, we can assume she was using nick name to apply for this competition, so in datasets `bake` and `baker`, the name cannot match with the rows in `results`. Also, the `series` information matches. We should do data cleaning to solve this issue. For simplicity, we will change the name in `results` dataset.\

After cleaning the data, we can see except for the `bakes` dataset only have limit information, all other `auto_join` results becomes 0, which means dataset can join perfectly with all information.\

Also, the order of joining dataset does matters! On my first try, I joint `bakes` and `bakers` first, and use this combined dataset to join the `results` dataset. But issue appears in the final dataset: since one baker may have multiple dishes, so we apply `left_join` on bakes and bakers, but the joint dataset will not contain information for bakers in series 9 and 10. Accordingly, joining this dataset with `results` will lead to no bakers information for all bakers in series 9 and 10. This is an interesting finding I noticed. So I switch my order of joining dataset: joining `bakers` and `results` first, ensure every bakers get their personal information. And join `bakes` at last. In this case, we can avoid information incompleteness.\

After getting this single, final dataset, we combine bakers' first name and last name together for simplicity. And then reorganize the order: first showing the series and episode, and then bakers' name and their dishes, with their ranking and results. Their personal information comes at last place. Last but not least, we can see this dataset is kind of redundant, because some bakers are OUT but their names still showing up in every episode of that series, we will delete these rows to keep our dataset simply and more logical. In this way, the last appearance of each baker is at the time they are OUT. One thing to mention, for baker Terry in series 9 episode 4, his result is "[a]" instead of in or out, but our information shows he is OUT in series 9 episode 5, which means we should also keep him in for episode 4.\

The final tidy dataset is called `bakers_all_info_tidy`, it has 710 rows and 10 columns: these columns showing the series, episode, bakers' names, name of the bake,show_stopper, baker's ranking, results,and baker's personal information including their age, occupation and hometown. 


```{r}
write_csv(bakers_all_info_tidy, "datasets_hw2/gbb_datasets/bakers_all_info_tidy.csv")
```

```{r}
bakers_all_info_tidy %>% 
  filter((result == "STAR BAKER"|result == "WINNER") & 
           (series == 5|series == 6|series == 7 |series == 8 
         |series ==9|series ==10)) %>% 
  select(series,episode,baker_name) %>% 
  pivot_wider(names_from = "episode",
              values_from = "baker_name") %>% 
  knitr::kable()

```
**Were there any predictable overall winners? Any surprises? **

We will discuss this question by series order:\
In series 5, winner is kind of surprise, it seems Richard Burr gots more times of star baker;\
In series 6, winner is predictable, Nadiya Hussain has been rated as star baker many times before.\
In series 7, winner is also predictable.\
In series 8, winner is also predictable.\
In series 9, winner is also predictable.\
In series 10, winner is surprising, he has not been rated as star baker before.


```{r}
viewership_df <- read_csv("datasets_hw2/gbb_datasets/viewers.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    series_1:series_10,
    names_to = "series",
    names_prefix = "series_",
    values_to = "viewership"
  ) %>% 
  mutate(series = as.integer(series)) %>% 
  select(series,everything()) %>% 
  arrange(series,episode)

viewership_df %>% head(n = 10)

viewership_df %>% filter(series == 1) %>% 
  pull(viewership) %>% mean(na.rm = T)

viewership_df %>% filter(series == 5) %>% 
  pull(viewership) %>% mean()
```
The average viewership in Season 1 is 2.77;\
And the average viewership in Season 5 is 10.0393.
